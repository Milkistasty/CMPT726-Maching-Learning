\question{Linear Regression [20 points]}

\textbf{List your collaborators on this question below. }

\begin{solution}

\paragraph{Students I got help from:} WRITE STUDENTS' NAMES HERE

\paragraph{Students I gave help to:} WRITE STUDENTS' NAMES HERE

\paragraph{External sources that I consulted (websites, books, notes, etc.):} LIST ALL EXTERNAL SOURCES HERE

\end{solution}

\qpart{[5 points]} 

Consider a vector $\vec{v} \in \mathbb{R}^{n}$, where $n \geq 2$. Given the first element of $\vec{v}$ and the differences between adjacent elements of $\vec{v}$, our goal is to reconstruct the vector. 

Formulate an ordinary least squares (OLS) problem such that the input data $A$ and desired outputs $\vec{y}$ are derived from the quantities we are given and the solution (i.e., $\vec{w}^* := \arg\min_{\vec{w}} \Vert A\vec{w} - \vec{y} \Vert_2^2$) can be turned into a reconstruction of the vector. \textbf{Write down what each element of $A$, $\vec{y}$ and $\vec{w}$ corresponds to. What are the dimensions of $A$, $y$ and $\vec{w}$?} Justify your answer. 

\begin{solution}

YOUR SOLUTIONS HERE

{\color{red} Final Conclusion: YOUR FINAL ANSWER OR CONCLUSION HERE}

\end{solution}

\qpart{[5 points]} 

Any grayscale image can be represented as a matrix $I$ where $I_{i,j}$ is the intensity of the pixel on the $i$th row and $j$th column. Consider an $n\times n$ image $I \in \mathbb{R}^{n \times n}$, where $n \geq 2$. Given the values of $I$ in the first column and the horizontal image gradients, which are the differences between values in adjacent columns, our goal is to reconstruct the full image. 

More precisely, we denote the horizontal image gradient at a pixel in the $i$th row and $j$th column as $\Delta^{x}_{i,j}$, which is defined to be $I_{i, j + 1} - I_{i, j}$. We are given $\Delta^{x}_{i,j}$ for all $i \in \{1, \ldots, n\}$ and $j \in \{1, \ldots, n - 1\}$ and $I_{i,1}$ for all $i \in \{1, \ldots, n\}$, and would like to find $I_{i,j}$ for all $i \in \{1, \ldots, n\}$ and $j \in \{2, \ldots, n\}$. 

Formulate an ordinary least squares (OLS) problem such that the input data $A$ and desired outputs $\vec{y}$ are derived from the quantities we are given and the solution (i.e., $\vec{w}^* := \arg\min_{\vec{w}} \Vert A\vec{w} - \vec{y} \Vert_2^2$) can be turned into a reconstruction of the image. \textbf{Write down what each element of $A$, $\vec{y}$ and $\vec{w}$ corresponds to. What are the dimensions of $A$, $y$ and $\vec{w}$?} Justify your answer. 

\begin{solution}

YOUR SOLUTIONS HERE

{\color{red} Final Conclusion: YOUR FINAL ANSWER OR CONCLUSION HERE}

\end{solution}

\qpart{[5 points]}

Unlike in the previous part, we are no longer given the values of $I$ in the first column. However, in addition to being given the horizontal image gradients, we are also given the vertical image gradients, which are the differences between values in adjacent rows. Our goal is still to reconstruct the full image. 

More precisely, we will denote horizontal image gradients in the same way as before, and the vertical image gradient at a pixel in the $i$th row and $j$th column as $\Delta^{y}_{i,j}$, which is defined to be $I_{i + 1, j} - I_{i, j}$. We are given $\Delta^{x}_{i,j}$ for all $i \in \{1, \ldots, n\}$ and $j \in \{1, \ldots, n - 1\}$, and $\Delta^{y}_{i,j}$ for all $i \in \{1, \ldots, n - 1\}$ and $j \in \{1, \ldots, n\}$. 

We are given $\Delta^{x}_{i,j}$ for all $i \in \{1, \ldots, n\}$ and $j \in \{1, \ldots, n - 1\}$ and $\Delta^{y}_{i,j}$ for all $i \in \{1, \ldots, n - 1\}$ and $j \in \{1, \ldots, n\}$, and would like to find $I_{i,j}$ for all $i \in \{1, \ldots, n\}$ and $j \in \{1, \ldots, n\}$. 

Formulate an ordinary least squares (OLS) problem such that the input data $A$ and desired outputs $\vec{y}$ are derived from the quantities we are given and the solution (i.e., $\vec{w}^* := \arg\min_{\vec{w}} \Vert A\vec{w} - \vec{y} \Vert_2^2$) can be turned into a reconstruction of the image. \textbf{Write down what each element of $A$, $\vec{y}$ and $\vec{w}$ corresponds to. What are the dimensions of $A$, $y$ and $\vec{w}$?} Justify your answer. 

\begin{solution}

YOUR SOLUTIONS HERE

{\color{red} Final Conclusion: YOUR FINAL ANSWER OR CONCLUSION HERE}

\end{solution}

\qpart{[5 points]} 
In the \texttt{x\_gradients.npy} and \texttt{y\_gradients.npy} files, you are given the horizontal image gradients and vertical image gradients of an unknown image. Write an implementation that constructs the input data matrix $A$ and the vector of desired outputs $\vec{y}$ used in the OLS formulation in the previous part. Then write an implementation that solves the OLS problem using the closed-form formula for the solution. You can only use functions that implement basic linear algebra operations, such as matrix multiplication, inverses, transposes and solving a linear system of equations where the number of equations is equal to the number of unknowns. You have been provided with starter code for loading and visualizing data. 

\begin{solution}

YOUR SOLUTIONS HERE

{\color{red} Final Conclusion: YOUR FINAL ANSWER OR CONCLUSION HERE}

\end{solution}

\qpart{[Bonus: 3 points]}
In one or two sentences comment on the memory requirement of the approach implemented for the previous part and if it can be reduced by considering the characteristics of input data matrix $A$.

\begin{solution}

YOUR SOLUTIONS HERE

{\color{red} Final Conclusion: YOUR FINAL ANSWER OR CONCLUSION HERE}

\end{solution}
